{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### An example of variational autoencoder on MNIST dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch [0/15], step [0/469], loss 69634.4375\n",
      "epoch [0/15], step [100/469], loss 20367.7637\n",
      "epoch [0/15], step [200/469], loss 14855.4141\n",
      "epoch [0/15], step [300/469], loss 13677.4385\n",
      "epoch [0/15], step [400/469], loss 12759.3438\n",
      "epoch [1/15], step [0/469], loss 12267.4014\n",
      "epoch [1/15], step [100/469], loss 12288.5322\n",
      "epoch [1/15], step [200/469], loss 11912.9238\n",
      "epoch [1/15], step [300/469], loss 11422.7402\n",
      "epoch [1/15], step [400/469], loss 11108.4932\n",
      "epoch [2/15], step [0/469], loss 11123.5107\n",
      "epoch [2/15], step [100/469], loss 10773.5293\n",
      "epoch [2/15], step [200/469], loss 10900.3799\n",
      "epoch [2/15], step [300/469], loss 10130.8721\n",
      "epoch [2/15], step [400/469], loss 10427.7109\n",
      "epoch [3/15], step [0/469], loss 10337.8076\n",
      "epoch [3/15], step [100/469], loss 10396.6172\n",
      "epoch [3/15], step [200/469], loss 9985.0762\n",
      "epoch [3/15], step [300/469], loss 10115.6514\n",
      "epoch [3/15], step [400/469], loss 9979.4248\n",
      "epoch [4/15], step [0/469], loss 9855.0234\n",
      "epoch [4/15], step [100/469], loss 9423.7207\n",
      "epoch [4/15], step [200/469], loss 10129.3906\n",
      "epoch [4/15], step [300/469], loss 9805.4805\n",
      "epoch [4/15], step [400/469], loss 9944.1523\n",
      "epoch [5/15], step [0/469], loss 10111.8613\n",
      "epoch [5/15], step [100/469], loss 9657.2646\n",
      "epoch [5/15], step [200/469], loss 9739.1592\n",
      "epoch [5/15], step [300/469], loss 9829.7705\n",
      "epoch [5/15], step [400/469], loss 9467.1250\n",
      "epoch [6/15], step [0/469], loss 9662.9766\n",
      "epoch [6/15], step [100/469], loss 10152.5244\n",
      "epoch [6/15], step [200/469], loss 9227.5947\n",
      "epoch [6/15], step [300/469], loss 9942.4385\n",
      "epoch [6/15], step [400/469], loss 9550.0703\n",
      "epoch [7/15], step [0/469], loss 9778.8848\n",
      "epoch [7/15], step [100/469], loss 10026.3516\n",
      "epoch [7/15], step [200/469], loss 9243.4619\n",
      "epoch [7/15], step [300/469], loss 9244.1133\n",
      "epoch [7/15], step [400/469], loss 9191.6055\n",
      "epoch [8/15], step [0/469], loss 8955.4307\n",
      "epoch [8/15], step [100/469], loss 9640.0400\n",
      "epoch [8/15], step [200/469], loss 8686.1328\n",
      "epoch [8/15], step [300/469], loss 9465.2764\n",
      "epoch [8/15], step [400/469], loss 9260.9131\n",
      "epoch [9/15], step [0/469], loss 9157.4863\n",
      "epoch [9/15], step [100/469], loss 9216.7139\n",
      "epoch [9/15], step [200/469], loss 9287.5303\n",
      "epoch [9/15], step [300/469], loss 9335.4268\n",
      "epoch [9/15], step [400/469], loss 8972.7686\n",
      "epoch [10/15], step [0/469], loss 9014.0420\n",
      "epoch [10/15], step [100/469], loss 8857.8457\n",
      "epoch [10/15], step [200/469], loss 9206.9248\n",
      "epoch [10/15], step [300/469], loss 9095.9658\n",
      "epoch [10/15], step [400/469], loss 8825.6309\n",
      "epoch [11/15], step [0/469], loss 8599.1475\n",
      "epoch [11/15], step [100/469], loss 9539.2070\n",
      "epoch [11/15], step [200/469], loss 9109.6270\n",
      "epoch [11/15], step [300/469], loss 8990.6543\n",
      "epoch [11/15], step [400/469], loss 9134.0078\n",
      "epoch [12/15], step [0/469], loss 8722.2568\n",
      "epoch [12/15], step [100/469], loss 8998.5742\n",
      "epoch [12/15], step [200/469], loss 8992.8135\n",
      "epoch [12/15], step [300/469], loss 8152.6011\n",
      "epoch [12/15], step [400/469], loss 9220.8770\n",
      "epoch [13/15], step [0/469], loss 8827.5518\n",
      "epoch [13/15], step [100/469], loss 8597.1494\n",
      "epoch [13/15], step [200/469], loss 9541.6865\n",
      "epoch [13/15], step [300/469], loss 8800.0176\n",
      "epoch [13/15], step [400/469], loss 9295.6592\n",
      "epoch [14/15], step [0/469], loss 8702.7783\n",
      "epoch [14/15], step [100/469], loss 8796.9033\n",
      "epoch [14/15], step [200/469], loss 9327.6924\n",
      "epoch [14/15], step [300/469], loss 8928.8975\n",
      "epoch [14/15], step [400/469], loss 9162.5391\n"
     ]
    }
   ],
   "source": [
    "import torch \n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn.functional as F\n",
    "from torchvision.utils import save_image\n",
    "import os\n",
    "\n",
    "'''move the computations to the GPU if cuda is available, otherwise the computations will be run on CPU'''\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "'''defining model parameters'''\n",
    "image_size = 784\n",
    "h_dim = 400\n",
    "z_dim = 20\n",
    "num_epochs = 15\n",
    "batch_size = 128\n",
    "learning_rate = 1e-3\n",
    "\n",
    "'''download the training and test set'''\n",
    "train_dataset = torchvision.datasets.MNIST(root='data/', \n",
    "                                           train=True, \n",
    "                                           transform=transforms.ToTensor(),  \n",
    "                                           download=True)\n",
    "\n",
    "test_dataset = torchvision.datasets.MNIST(root='data/', \n",
    "                                          train=False, \n",
    "                                          transform=transforms.ToTensor())\n",
    "\n",
    "'''use dataloader to shuffle and batch the data'''\n",
    "train_loader = torch.utils.data.DataLoader(dataset = train_dataset, batch_size = batch_size, shuffle = True)\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "'''define the model'''\n",
    "class vae(nn.Module):\n",
    "    def __init__(self, image_size = 784, h_dim = 400, z_dim = 20):\n",
    "        super(vae, self).__init__()\n",
    "        self.layer_1 = nn.Linear(image_size, h_dim)\n",
    "        self.layer_2_1 = nn.Linear(h_dim, z_dim)\n",
    "        self.layer_2_2 = nn.Linear(h_dim, z_dim)\n",
    "        self.layer_3 = nn.Linear(z_dim, h_dim)\n",
    "        self.layer_4 = nn.Linear(h_dim, image_size)\n",
    "        self.relu = nn.ReLU()\n",
    "    \n",
    "    def encoder(self, x):\n",
    "        out = self.layer_1(x)\n",
    "        out = self.relu(out)\n",
    "        mu = self.layer_2_1(out)\n",
    "        std = self.layer_2_2(out)\n",
    "        return mu, std\n",
    "\n",
    "    def decoder(self, z):\n",
    "        out = self.layer_3(z)\n",
    "        out = self.relu(out)\n",
    "        out = self.layer_4(out)\n",
    "        out = torch.sigmoid(out)\n",
    "        return out\n",
    "    \n",
    "    def forward(self, x):\n",
    "        mu, std = self.encoder(x)\n",
    "        z = self.parameterize(mu, std)\n",
    "        out = self.decoder(z)\n",
    "        return mu, std, out\n",
    "    \n",
    "    def parameterize(self, mu, std):\n",
    "        #Returns a tensor with the same size as input that is filled with random numbers \n",
    "        #from a normal distribution with mean 0 and variance 1\n",
    "        sample = torch.randn_like(std)\n",
    "        out = mu + (std*sample)\n",
    "        return out\n",
    "\n",
    "'''instantiate the model'''\n",
    "model = vae().to(device)\n",
    "#criterion = kl_divergence + loss_of_generator\n",
    "'''Adam optimizer is used as the optimization function. We optimized all the model parameters, with a given learning rate.'''\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = learning_rate)\n",
    "\n",
    "'''training'''\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (img, label) in enumerate(train_loader):\n",
    "        img = img.reshape(-1, image_size).to(device)\n",
    "        mu, std, out = model(img)\n",
    "        \n",
    "        gen_loss = F.binary_cross_entropy(out, img, size_average=False)\n",
    "        \n",
    "        kl_div = - 0.5 * torch.sum(1 + std - mu.pow(2) - std.exp())\n",
    "        \n",
    "        loss = gen_loss + kl_div\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if i%100 == 0:\n",
    "            print('epoch [{}/{}], step [{}/{}], loss {:.4f}'.format(epoch, num_epochs, i, len(train_loader), loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from torchvision.utils import save_image\n",
    "import os\n",
    "from PIL import Image\n",
    "\n",
    "sample_dir = \"data/\"\n",
    "\n",
    "with torch.no_grad():\n",
    "    z = torch.randn(batch_size, z_dim).to(device)\n",
    "    out = model.decoder(z).view(-1, 1, 28, 28)\n",
    "    save_image(out, os.path.join(sample_dir, 'sampled-{}.png'.format(epoch+1)))\n",
    "    img = Image.open(os.path.join(sample_dir, 'sampled-{}.png'.format(epoch+1)))\n",
    "    img.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
