{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### An example of variational autoencoder on MNIST dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch [0/15], step [0/469], loss 69694.5703\n",
      "epoch [0/15], step [100/469], loss 21035.7832\n",
      "epoch [0/15], step [200/469], loss 16209.1182\n",
      "epoch [0/15], step [300/469], loss 13802.0586\n",
      "epoch [0/15], step [400/469], loss 12742.9785\n",
      "epoch [1/15], step [0/469], loss 11931.6426\n",
      "epoch [1/15], step [100/469], loss 11973.4199\n",
      "epoch [1/15], step [200/469], loss 11213.8047\n",
      "epoch [1/15], step [300/469], loss 10665.0469\n",
      "epoch [1/15], step [400/469], loss 11209.6924\n",
      "epoch [2/15], step [0/469], loss 10739.9209\n",
      "epoch [2/15], step [100/469], loss 10400.5293\n",
      "epoch [2/15], step [200/469], loss 10643.6660\n",
      "epoch [2/15], step [300/469], loss 10382.5850\n",
      "epoch [2/15], step [400/469], loss 10576.7363\n",
      "epoch [3/15], step [0/469], loss 10327.3721\n",
      "epoch [3/15], step [100/469], loss 10249.4600\n",
      "epoch [3/15], step [200/469], loss 10094.6299\n",
      "epoch [3/15], step [300/469], loss 10378.8857\n",
      "epoch [3/15], step [400/469], loss 9671.6836\n",
      "epoch [4/15], step [0/469], loss 10149.3115\n",
      "epoch [4/15], step [100/469], loss 10028.2197\n",
      "epoch [4/15], step [200/469], loss 9827.4648\n",
      "epoch [4/15], step [300/469], loss 9986.1680\n",
      "epoch [4/15], step [400/469], loss 9762.3887\n",
      "epoch [5/15], step [0/469], loss 9619.0801\n",
      "epoch [5/15], step [100/469], loss 9754.6289\n",
      "epoch [5/15], step [200/469], loss 9722.0742\n",
      "epoch [5/15], step [300/469], loss 9512.3818\n",
      "epoch [5/15], step [400/469], loss 9261.2715\n",
      "epoch [6/15], step [0/469], loss 10055.5244\n",
      "epoch [6/15], step [100/469], loss 9714.3857\n",
      "epoch [6/15], step [200/469], loss 9253.9629\n",
      "epoch [6/15], step [300/469], loss 9433.0010\n",
      "epoch [6/15], step [400/469], loss 9113.0674\n",
      "epoch [7/15], step [0/469], loss 9113.4824\n",
      "epoch [7/15], step [100/469], loss 9925.6797\n",
      "epoch [7/15], step [200/469], loss 9314.9346\n",
      "epoch [7/15], step [300/469], loss 9485.8262\n",
      "epoch [7/15], step [400/469], loss 9368.9121\n",
      "epoch [8/15], step [0/469], loss 9170.4717\n",
      "epoch [8/15], step [100/469], loss 9185.4180\n",
      "epoch [8/15], step [200/469], loss 9240.7803\n",
      "epoch [8/15], step [300/469], loss 9588.8340\n",
      "epoch [8/15], step [400/469], loss 9224.1318\n",
      "epoch [9/15], step [0/469], loss 9136.4141\n",
      "epoch [9/15], step [100/469], loss 9135.8730\n",
      "epoch [9/15], step [200/469], loss 8924.7188\n",
      "epoch [9/15], step [300/469], loss 8839.4209\n",
      "epoch [9/15], step [400/469], loss 8878.9629\n",
      "epoch [10/15], step [0/469], loss 9540.9609\n",
      "epoch [10/15], step [100/469], loss 8769.4023\n",
      "epoch [10/15], step [200/469], loss 9384.0723\n",
      "epoch [10/15], step [300/469], loss 8961.4922\n",
      "epoch [10/15], step [400/469], loss 9277.4443\n",
      "epoch [11/15], step [0/469], loss 9088.1211\n",
      "epoch [11/15], step [100/469], loss 8947.8643\n",
      "epoch [11/15], step [200/469], loss 9202.6699\n",
      "epoch [11/15], step [300/469], loss 9130.1787\n",
      "epoch [11/15], step [400/469], loss 9099.3799\n",
      "epoch [12/15], step [0/469], loss 9161.4336\n",
      "epoch [12/15], step [100/469], loss 8987.7871\n",
      "epoch [12/15], step [200/469], loss 9012.6602\n",
      "epoch [12/15], step [300/469], loss 8945.8047\n",
      "epoch [12/15], step [400/469], loss 8975.8213\n",
      "epoch [13/15], step [0/469], loss 9440.0371\n",
      "epoch [13/15], step [100/469], loss 9134.5088\n",
      "epoch [13/15], step [200/469], loss 8521.4014\n",
      "epoch [13/15], step [300/469], loss 8568.1572\n",
      "epoch [13/15], step [400/469], loss 8873.3037\n",
      "epoch [14/15], step [0/469], loss 9075.9307\n",
      "epoch [14/15], step [100/469], loss 8646.0332\n",
      "epoch [14/15], step [200/469], loss 8833.3545\n",
      "epoch [14/15], step [300/469], loss 9086.3525\n",
      "epoch [14/15], step [400/469], loss 8828.9102\n"
     ]
    }
   ],
   "source": [
    "import torch \n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn.functional as F\n",
    "from torchvision.utils import save_image\n",
    "import os\n",
    "\n",
    "'''move the computations to the GPU if cuda is available, otherwise the computations will be run on CPU'''\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "'''defining model parameters'''\n",
    "image_size = 784\n",
    "h_dim = 400\n",
    "z_dim = 20\n",
    "num_epochs = 15\n",
    "batch_size = 128\n",
    "learning_rate = 1e-3\n",
    "\n",
    "'''download the training and test set'''\n",
    "train_dataset = torchvision.datasets.MNIST(root='data/', \n",
    "                                           train=True, \n",
    "                                           transform=transforms.ToTensor(),  \n",
    "                                           download=True)\n",
    "\n",
    "test_dataset = torchvision.datasets.MNIST(root='data/', \n",
    "                                          train=False, \n",
    "                                          transform=transforms.ToTensor())\n",
    "\n",
    "'''use dataloader to shuffle and batch the data'''\n",
    "train_loader = torch.utils.data.DataLoader(dataset = train_dataset, batch_size = batch_size, shuffle = True)\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "'''define the model'''\n",
    "class vae(nn.Module):\n",
    "    def __init__(self, image_size = 784, h_dim = 400, z_dim = 20):\n",
    "        super(vae, self).__init__()\n",
    "        self.layer_1 = nn.Linear(image_size, h_dim)\n",
    "        self.layer_2_1 = nn.Linear(h_dim, z_dim)\n",
    "        self.layer_2_2 = nn.Linear(h_dim, z_dim)\n",
    "        self.layer_3 = nn.Linear(z_dim, h_dim)\n",
    "        self.layer_4 = nn.Linear(h_dim, image_size)\n",
    "        self.relu = nn.ReLU()\n",
    "    \n",
    "    def encoder(self, x):\n",
    "        out = self.layer_1(x)\n",
    "        out = self.relu(out)\n",
    "        mu = self.layer_2_1(out)\n",
    "        std = self.layer_2_2(out)\n",
    "        return mu, std\n",
    "\n",
    "    def decoder(self, z):\n",
    "        out = self.layer_3(z)\n",
    "        out = self.relu(out)\n",
    "        out = self.layer_4(out)\n",
    "        out = torch.sigmoid(out)\n",
    "        return out\n",
    "    \n",
    "    def forward(self, x):\n",
    "        mu, std = self.encoder(x)\n",
    "        z = self.parameterize(mu, std)\n",
    "        out = self.decoder(z)\n",
    "        return mu, std, out\n",
    "    \n",
    "    def parameterize(self, mu, std):\n",
    "        #Returns a tensor with the same size as input that is filled with random numbers \n",
    "        #from a normal distribution with mean 0 and variance 1\n",
    "        sample = torch.randn_like(std)\n",
    "        out = mu + (std*sample)\n",
    "        return out\n",
    "\n",
    "'''instantiate the model'''\n",
    "model = vae().to(device)\n",
    "#criterion = kl_divergence + loss_of_generator\n",
    "'''Adam optimizer is used as the optimization function. We optimized all the model parameters, with a given learning rate.'''\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = learning_rate)\n",
    "\n",
    "'''training'''\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (img, label) in enumerate(train_loader):\n",
    "        img = img.reshape(-1, image_size).to(device)\n",
    "        mu, std, out = model(img)\n",
    "        \n",
    "        gen_loss = F.binary_cross_entropy(out, img, size_average=False)\n",
    "        \n",
    "        kl_div = - 0.5 * torch.sum(1 + std - mu.pow(2) - std.exp())\n",
    "        \n",
    "        loss = gen_loss + kl_div\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if i%100 == 0:\n",
    "            print('epoch [{}/{}], step [{}/{}], loss {:.4f}'.format(epoch, num_epochs, i, len(train_loader), loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.utils import save_image\n",
    "import os\n",
    "from PIL import Image\n",
    "\n",
    "sample_dir = \"data/\"\n",
    "\n",
    "with torch.no_grad():\n",
    "    z = torch.randn(batch_size, z_dim).to(device)\n",
    "    out = model.decoder(z).view(-1, 1, 28, 28)\n",
    "    save_image(out, os.path.join(sample_dir, 'sampled-{}.png'.format(epoch+1)))\n",
    "    img = Image.open(os.path.join(sample_dir, 'sampled-{}.png'.format(epoch+1)))\n",
    "    img.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
