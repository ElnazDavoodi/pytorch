{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### An example of bidirectional RNN on MNIST dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch [1/2] step [1/600], loss 2.2995\n",
      "epoch [1/2] step [101/600], loss 0.6912\n",
      "epoch [1/2] step [201/600], loss 0.2863\n",
      "epoch [1/2] step [301/600], loss 0.2609\n",
      "epoch [1/2] step [401/600], loss 0.1255\n",
      "epoch [1/2] step [501/600], loss 0.0634\n",
      "epoch [2/2] step [1/600], loss 0.0367\n",
      "epoch [2/2] step [101/600], loss 0.1528\n",
      "epoch [2/2] step [201/600], loss 0.0911\n",
      "epoch [2/2] step [301/600], loss 0.1258\n",
      "epoch [2/2] step [401/600], loss 0.0789\n",
      "epoch [2/2] step [501/600], loss 0.0816\n"
     ]
    }
   ],
   "source": [
    "import torch \n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "'''move the computations to the GPU if cuda is available, otherwise the computations will be run on CPU'''\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "'''defining model parameters'''\n",
    "sequence_length = 28\n",
    "input_size = 28\n",
    "hidden_size = 128\n",
    "num_layers = 2\n",
    "num_classes = 10\n",
    "batch_size = 100\n",
    "num_epochs = 2\n",
    "learning_rate = 0.003\n",
    "\n",
    "'''download the training and test set'''\n",
    "train_dataset = torchvision.datasets.MNIST(root='data/', \n",
    "                                           train=True, \n",
    "                                           transform=transforms.ToTensor(),  \n",
    "                                           download=True)\n",
    "\n",
    "test_dataset = torchvision.datasets.MNIST(root='data/', \n",
    "                                          train=False, \n",
    "                                          transform=transforms.ToTensor())\n",
    "\n",
    "'''use dataloader to shuffle and batch the data'''\n",
    "train_loader = torch.utils.data.DataLoader(dataset = train_dataset, batch_size = batch_size, shuffle = True)\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "'''define the model'''\n",
    "class biLSTM(nn.Module):\n",
    "    def __init__(self, input_size, sequence_length, hidden_size, num_layers, num_classes):\n",
    "        super(biLSTM, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True, bidirectional=True)\n",
    "        self.fc = nn.Linear(2*hidden_size, num_classes)\n",
    "        \n",
    "    def forward(self, input):\n",
    "        '''initialize the hidden state and cell state as zero'''\n",
    "        h0 = torch.zeros(self.num_layers*2, input.size(0), self.hidden_size)\n",
    "        c0 = torch.zeros(self.num_layers*2, input.size(0), self.hidden_size)\n",
    "        '''output: tensor of shape (batch_size, seq_length, hidden_size*2)'''\n",
    "        output,_ = self.lstm(input, (h0, c0))\n",
    "        '''get the output of the last time step'''\n",
    "        '''output(100, 28, 256(2*128))'''\n",
    "        output = self.fc(output[:,-1,:])\n",
    "        return output\n",
    "\n",
    "'''instantiate the model'''\n",
    "model = biLSTM(input_size, sequence_length, hidden_size, num_layers, num_classes).to(device)\n",
    "\n",
    "'''cross entropy is used as loss function'''\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "'''Adam optimizer is used as the optimization function. We optimized all the model parameters, with a given learning rate.'''\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "'''training'''\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (img, label) in enumerate(train_loader):\n",
    "        img = img.reshape(-1, 28, 28)\n",
    "        img = img.to(device)\n",
    "        pred = model(img)\n",
    "        label = label.to(device)\n",
    "        \n",
    "        loss = criterion(pred, label)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if i%100==0:\n",
    "            print('epoch [{}/{}] step [{}/{}], loss {:.4f}'.format(epoch+1, num_epochs, i+1, len(train_loader),loss))\n",
    "\n",
    "'''calculate the performance of the trained model on unseen test set.'''\n",
    "true = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for img, label in test_loader:\n",
    "        img = img.reshape(-1, 28, 28).to(device)\n",
    "        pred = model(img)\n",
    "        prob, pred_label = torch.max(pred.data, 1)\n",
    "        true +=(pred_label==label).sum()\n",
    "        total += label.size(0)\n",
    "print('accuracy on the test set is {} percent'.format(100 * float(true)/total))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
