{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This is an example of application of neural networks to build a language model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch [1/5], step [1/159], loss 8.1972\n",
      "epoch [1/5], step [101/159], loss 5.9496\n",
      "epoch [2/5], step [1/159], loss 5.5469\n",
      "epoch [2/5], step [101/159], loss 4.9154\n",
      "epoch [3/5], step [1/159], loss 4.6518\n",
      "epoch [3/5], step [101/159], loss 4.0222\n",
      "epoch [4/5], step [1/159], loss 3.5502\n",
      "epoch [4/5], step [101/159], loss 3.2595\n",
      "epoch [5/5], step [1/159], loss 2.7021\n",
      "epoch [5/5], step [101/159], loss 2.5975\n"
     ]
    }
   ],
   "source": [
    "import torch \n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "'''move the computations to the GPU if cuda is available, otherwise the computations will be run on CPU'''\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "'''defining model parameters'''\n",
    "embed_size = 128\n",
    "hidden_size = 1024\n",
    "num_layers = 1\n",
    "num_epochs = 5\n",
    "num_samples = 1000     # number of words to be sampled\n",
    "batch_size = 20\n",
    "seq_length = 10\n",
    "learning_rate = 0.002\n",
    "\n",
    "'''pre-process the data'''\n",
    "class dictionary():\n",
    "    def __init__(self):\n",
    "        self.word_to_idx = {'eos':0}\n",
    "        self.idx_to_word = {0:'eos'}\n",
    "        self.vocab_size = 1\n",
    "        self.token_ids = []\n",
    "    def add_word(self, word):\n",
    "        if word not in self.word_to_idx:\n",
    "            self.word_to_idx[word] = len(self.word_to_idx)\n",
    "            self.idx_to_word[len(self.idx_to_word)] = word\n",
    "            self.vocab_size +=1\n",
    "        self.token_ids.append(self.word_to_idx[word])\n",
    "class document():\n",
    "    def __init__(self):\n",
    "        self.corpus = dictionary()\n",
    "    def build_corpus(self):\n",
    "        input_text = open('data/filter_text.txt').readlines()\n",
    "        for i in range(len(input_text)):\n",
    "            input_text[i] = input_text[i].strip()\n",
    "            input_text[i] += ' eos '\n",
    "        input_string = ''.join(input_text)\n",
    "        for word in input_string.split(' '):\n",
    "            self.corpus.add_word(word)\n",
    "        return self.corpus\n",
    "    \n",
    "        \n",
    "'''the dataset used is a sample of 500 reviews from Amazon Review Dataset. The reviews are cleaned during pre-processing.'''\n",
    "corpus = document()\n",
    "text_dict = corpus.build_corpus()\n",
    "\n",
    "inputs = []\n",
    "outputs = []\n",
    "num_inputs = int(len(text_dict.token_ids)/seq_length)\n",
    "for i in range(num_inputs):\n",
    "    inputs.append(text_dict.token_ids[i*seq_length:(i+1)*seq_length])\n",
    "    outputs.append(text_dict.token_ids[(i*seq_length)+1:((i+1)*seq_length)+1])\n",
    "    \n",
    "'''define the model: a recurrent NN'''\n",
    "class LM(nn.Module):\n",
    "    def __init__(self, vocab_size, num_layers, hidden_size, embed_size):\n",
    "        super(LM, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.embed_layer = nn.Embedding(vocab_size, embed_size)\n",
    "        self.lstm = nn.LSTM(embed_size, hidden_size, num_layers, batch_first = True)\n",
    "        self.linear = nn.Linear(hidden_size, vocab_size)\n",
    "    def forward(self, input):\n",
    "        h0 = torch.zeros(self.num_layers, input.size(0), self.hidden_size)\n",
    "        c0 = torch.zeros(self.num_layers, input.size(0), self.hidden_size)\n",
    "        out = self.embed_layer(input)\n",
    "        out, _ = self.lstm(out, (h0, c0))\n",
    "#         print(out.shape)\n",
    "        #Reshape output to (batch_size*sequence_length, hidden_size)\n",
    "        out = out.reshape(out.size(0)*out.size(1), out.size(2))\n",
    "#         print(out.shape)\n",
    "        out = self.linear(out)\n",
    "        return out\n",
    "\n",
    "'''instantiate the model'''\n",
    "model = LM(text_dict.vocab_size, num_layers, hidden_size, embed_size).to(device)\n",
    "\n",
    "'''cross entropy is used as loss function'''\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "'''Adam optimizer is used as the optimization function. We optimized all the model parameters, with a given learning rate.'''\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "'''training...'''\n",
    "for epoch in range(num_epochs):\n",
    "    for i in range(int(num_inputs/batch_size)):\n",
    "        input_tensor = torch.LongTensor(np.array(inputs[i*batch_size:(i+1)*batch_size])).to(device)\n",
    "        output_tensor = torch.LongTensor(np.array(outputs[i*batch_size:(i+1)*batch_size])).to(device)\n",
    "\n",
    "        pred = model(input_tensor)\n",
    "#         print(pred.shape, output_tensor.shape)\n",
    "        '''\n",
    "        output and pred are in the size of [batch_size*seq_len, vocab size], \n",
    "        which means for each word in the sequence of each batch, we generate a probability distribution over vocabulary\n",
    "        ''' \n",
    "        \n",
    "        loss = criterion(pred, output_tensor.reshape(-1))\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if i%100==0:\n",
    "            print('epoch [{}/{}], step [{}/{}], loss {:.4f}'.format(epoch+1, num_epochs, i+1, int(num_inputs/batch_size), loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bushel\n",
      "box\n"
     ]
    }
   ],
   "source": [
    "'''sample a word from a dictionary randomly and get the next word'''\n",
    "with torch.no_grad():\n",
    "    prob = torch.ones(text_dict.vocab_size)\n",
    "    input = torch.multinomial(prob, num_samples=1).unsqueeze(1).to(device)\n",
    "    print(text_dict.idx_to_word[input.item()])\n",
    "    output = model(input)\n",
    "    _,output = torch.max(output, 1)\n",
    "    print(text_dict.idx_to_word[output.item()])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
